import os
import pickle
import sys
import re
import torch
import numpy as np
import pandas as pd
import torch.nn.functional as F
import spacy
from leven import levenshtein

sys.path.append("qulog_linguistic_attention/classes")

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

from collections import defaultdict
from typing import List
from .classes.tokenizer import *

PAD_SIZE = 16

# !!! Autogenerated !!! Do not change. Path where this python file is located
home = os.path.dirname(os.path.realpath(__file__))
#
# data_map_ = {
# "A": "ROOT",
# "B": "acl",
# "C": "advcl",
# "D": "advmod",
# "E": "amod",
# "F": "appos",
# "G": "aux",
# "H": "auxpass",
# "I": "cc",
# "J": "compound",
# "K": "csubj",
# "L": "dep",
# "M": "det",
# "N": "dobj",
# "P": "intj",
# "Q": "mark",
# "R": "neg",
# "S": "nmod",
# "T": "npadvmod",
# "U": "nsubj",
# "V": "nsubjpass",
# "W": "pobj",
# "X": "prep",
# "Y": "prt",
# "Z": "punct",
# "a": "xcomp",
# }
#
# inverse_data_map_ = {'ROOT': 'A',
#  'acl': 'B',
#  'advcl': 'C',
#  'advmod': 'D',
#  'amod': 'E',
#  'appos': 'F',
#  'aux': 'G',
#  'auxpass': 'H',
#  'cc': 'I',
#  'compound': 'J',
#  'csubj': 'K',
#  'dep': 'L',
#  'det': 'M',
#  'dobj': 'N',
#  'intj': 'P',
#  'mark': 'Q',
#  'neg': 'R',
#  'nmod': 'S',
#  'npadvmod': 'T',
#  'nsubj': 'U',
#  'nsubjpass': 'V',
#  'pobj': 'W',
#  'prep': 'X',
#  'prt': 'Y',
#  'punct': 'Z',
#  'xcomp': 'a'}

inverse_data_map_ = {'ADJ':"A",
                     'ADP':"B",
                     'ADV':"C",
                    'AUX':"D",
                    'DET':"E",
                    'NOUN':"F",
                    'PART':"G",
                    'PROPN':"H",
                    'PUNCT':"I",
                    'SCONJ':"J",
                    'SYM':"K",
                    'VERB':"L",
                    'X':"M"}

data_map_ = {'A': 'ADJ',
 'B': 'ADP',
 'C': 'ADV',
 'D': 'AUX',
 'E': 'DET',
 'F': 'NOUN',
 'G': 'PART',
 'H': 'PROPN',
 'I': 'PUNCT',
 'J': 'SCONJ',
 'K': 'SYM',
 'L': 'VERB',
 'M': 'X'}

# good_data = np.array(['AAJN', 'AAJW', 'ABJJJF', 'ABLF', 'ABNAJW', 'AE', 'AEEJN', 'AEEN',
#        'AEJF', 'AEJN', 'AEJNZ', 'AELN', 'AEN', 'AENLZ', 'AENZ', 'AET',
#        'AJJF', 'AJJJJN', 'AJJJN', 'AJJN', 'AJJNXWZ', 'AJJNZ', 'AJN',
#        'AJNJL', 'AJNJN', 'AJNLL', 'AJNLLL', 'AJNX', 'AJNXW', 'AJNXWZ',
#        'AJNZ', 'AJNZXW', 'AJNZZ', 'ALJLZ', 'ALJN', 'ALJNZ', 'ALL',
#        'ALLLZ', 'ALLZ', 'ALN', 'ALP', 'ALPLL', 'ALXWZ', 'ALZ', 'AMEN',
#        'AMJN', 'AMJNZXWZLMJN', 'AMN', 'ANBNXWZ', 'ANE', 'ANJN', 'ANL',
#        'ANLL', 'ANLZ', 'ANN', 'ANNJF', 'ANNLLL', 'ANNXW', 'ANNZ', 'ANNZZ',
#        'ANP', 'ANX', 'ANXJW', 'ANXN', 'ANXW', 'ANXWZ', 'ANXZ', 'ANY',
#        'ANZ', 'ANZZ', 'APL', 'APLN', 'APLNZ', 'APLZ', 'APT', 'APZ',
#        'AQCJN', 'AQCN', 'AQCNN', 'AQUC', 'ASENZ', 'ASJF', 'ASJJN', 'ASJN',
#        'ASLL', 'ASLLLL', 'ASSJN', 'ASSL', 'ASSZ', 'ASW', 'ATZ', 'AU',
#        'AXEW', 'AXJFJF', 'AXJJJF', 'AXJW', 'AXLJJF', 'AXW', 'AYJN', 'AYL',
#        'AYN', 'AZLZ', 'AZNZ', 'AZZ', 'AZZL', 'AZZNZ', 'AaEN', 'AaJJN',
#        'AaJN', 'AaJNZ', 'AaLZ', 'AaN', 'AaNZ', 'DA', 'EAA', 'EAJN', 'EAN',
#        'EASSZ', 'EAX', 'EAXA', 'EAXEW', 'EAXJW', 'EAXW', 'EAXWZ', 'EAXZ',
#        'EAZ', 'EAZZ', 'EEAZ', 'EEEA', 'EEJA', 'EEJAZ', 'EEJJJA', 'EENAZ',
#        'EJAAA', 'EJALSL', 'EJASSL', 'EJAX', 'EJAXWZ', 'EJAZ', 'EJAZXW',
#        'EJAZZ', 'EJJA', 'EJJAXWZ', 'EJJAZ', 'EJJJA', 'EJJJJA', 'EJJJUA',
#        'EJNA', 'ELA', 'ELJA', 'ELJJA', 'ELLA', 'ELLJA', 'ENA', 'EPA',
#        'EPAZ', 'EPJA', 'EPNA', 'EPNJA', 'ESJA', 'ESNA', 'EUA', 'EYA',
#        'EZA', 'EZAZ', 'GAN', 'GRAJN', 'GRAJNZ', 'GRAN', 'GRANN', 'GRANNZ',
#        'GRANZ', 'GSA', 'ISA', 'JAJN', 'JAJNZ', 'JANXW', 'JANZ', 'JASLA',
#        'JASSL', 'JAXW', 'JAXWZ', 'JAZ', 'JAZAL', 'JAZLA', 'JEEA', 'JEJA',
#        'JJAXW', 'JJAZ', 'JJJA', 'JJJAZ', 'JJJJA', 'JJJJJA', 'JJJJUA',
#        'JJUA', 'JLA', 'JLJA', 'JUA', 'JUAXW', 'JUAZ', 'JUGA', 'JULA',
#        'JVHA', 'KYNA', 'LLA', 'LLJA', 'LLLA', 'LPJA', 'LUA', 'LUAZ',
#        'LZA', 'SEAZ', 'SEJA', 'SJJA', 'SJJJA', 'SLAZ', 'SLEJA', 'SLJJUA',
#        'SNEJJA', 'SNJA', 'SQCA', 'SSAZ', 'SZA', 'SZAZ', 'TAZ', 'UA',
#        'UAJN', 'UAXL', 'UAXN', 'UAXW', 'UAZ', 'UEJA', 'UGA', 'UGRA',
#        'ULA', 'ULJA', 'URA', 'UZA', 'VHA', 'VHRA', 'ZAN', 'ZANZ', 'ZAU',
#        'ZEA', 'ZJA', 'ZJAZ', 'ZUAZ'])

good_data = np.array(['A', 'AAF', 'AFFF', 'AFFFI', 'AFFI', 'AFI', 'AFL', 'ALFF', 'DGLF',
       'DGLFF', 'DGLFFI', 'DGLFI', 'DLF', 'FA', 'FBF', 'FDGL', 'FDL',
       'FFDL', 'FFF', 'FFFBF', 'FFFF', 'FFFFF', 'FFFFFF', 'FFFFFL',
       'FFFFI', 'FFFI', 'FFFL', 'FFI', 'FFL', 'FFLI', 'FFMMM', 'FGL',
       'FIL', 'FJFF', 'FJLFF', 'FL', 'FLFF', 'FLI', 'FMMM', 'IFFI', 'ILF',
       'L', 'LAAF', 'LAF', 'LAFF', 'LAFFI', 'LAFI', 'LBF', 'LBFF', 'LBFL',
       'LC', 'LEF', 'LEFF', 'LEFFIBFIEEFF', 'LFB', 'LFBF', 'LFBFF',
       'LFBFI', 'LFBI', 'LFF', 'LFFB', 'LFFBF', 'LFFBFI', 'LFFF',
       'LFFFBFI', 'LFFFF', 'LFFFFF', 'LFFFI', 'LFFI', 'LFFIBF', 'LFFII',
       'LFFMMM', 'LFI', 'LFII', 'LFL', 'LH', 'LHI', 'LI', 'LIFI', 'LII',
       'LKF', 'LLF', 'LLFF', 'LLFFF', 'LLFFI', 'LLFI', 'MMMM'])

# !!! Autogenerated !!! Do not change the class name
class QulogLinguisticAttention:
    """
    __author__: Bogatinovski Jasmin
    __email__: jasmin.bogatinovski@gmail.com

    This class implements the linguistic quality quantification functionality of the QuLog method
    (Stands for Quality Log Message). It implements a neural network to generate a score for the linguistic
    sufficient structure given an input log message.
    The score is bounded in the interval [0, 1]. Greater values indicate bad log messages.
    The score is not guaranteed to be monotonic. Therefore, claims of the form "log message 1 has linguistic score 0.7, while
    log message 2 linguistic score of 0.9 means that log message 1 is better than log message 2" is not justified.
    Although, our experimental resulted in characterizing the score into three groups:
        1) 0.0-0.2 being "messages with sufficient quality"
        2) 0.2-0.5 being log "messages with so-so quality"
        3) 0.5-1.0 being log "messages with bad quality"

    NOTES:
        1) pad_len (mandatory parameters) is a parameter corresponding to the model size.
        Consult the model owner for its value (or find it by yourself from the nn.Embedding layers of the model);
        2) batch_size (optional) is a parameter of arbitrary value. It controls the speed of processing the log messages.
        If you have many log messages you can increase this parameter.
    """

    def __init__(self, threshold=0.9):
        """
        :param threshold: This parameters is used to put a threshold over the uncertainity estimate score.
        The experiments performed on open-source projects on source code logs and production code logs suggest that
        values around 0.9 - 0.98 can improve on the performance of the maximal probability estimator.
        In case both decision procedures lead to different results, exploit further the situation.
        """
        # !!! Autogenerated !!! Do not change.
        with open("./qulog_linguistic_attention/tokenizer.pickle", 'rb') as file:
            self.tokenizer = pickle.load(file)
        self.model = self.load(os.path.join(home, 'model.pth'))

        # self.nlp = spacy.load('en_core_web_trf')
        self.nlp_parse = spacy.load('en_core_web_sm')
        self.pad_len = 16
        self.batch_size = 256
        self.results = []
        self.predictions = []

    # !!! Autogenerated !!! Do not change the method name.
    def load(self, model_file_path):
        '''TODO Implement'''
        return torch.load(model_file_path, map_location=torch.device('cpu'))

    # !!! Autogenerated !!! Do not change the method name.
    def predict(self, log_line: str):
        '''TODO Implement'''
        return self.predict_batch([log_line])[0]
    # !!! Autogenerated !!! Do not change the method name.

    def _create_test_data_loaders(self, load_test, pad_len, batch_size):
        test_data = TensorDataset(torch.tensor(self._get_padded_data(load_test), dtype=torch.int32))
        test_sampler = SequentialSampler(test_data)
        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)
        return test_dataloader

    # def _get_padded_data(self, data, pad_len):
    #     padded_seq = pad_sequences(data, maxlen=pad_len, dtype="long", truncating="post", padding="post")
    #     return padded_seq

    def _get_padded_data(self, tokenized):
        padded = torch.nn.utils.rnn.pad_sequence(tokenized, batch_first=True)
        return padded

    def _get_pos_embedding(self, doc):
        return doc.pos_

    def _get_pos_embeddings(self, trf_docs):
        embeddings = []
        for doc in trf_docs:
            embeddings.append(self._get_pos_embedding(doc))
        return embeddings

    def _get_word_class_embeddings(self, log_lines):
        docs = list(self.nlp_parse.pipe(log_lines))
        word_classses = [" ".join([t.pos_ for t in doc]) for doc in docs]
        # print(word_classses)
        return word_classses

    def _get_word_class_embeddings_dep(self, log_lines):
        docs = list(self.nlp_parse.pipe(log_lines))
        word_classses = [" ".join([t.pos_ for t in doc]) for doc in docs]
        # print(word_classses)
        return word_classses

    def _lev_metric(self, good_data, bad_data):
        distances = defaultdict()
        for x in good_data:
            distances[x] = levenshtein(x, bad_data)
        return distances

    def _predict(self, dataloader):
        self.model.eval()
        preds = []
        tmps = []
        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                load = batch[0]
                out = self.model.forward(load.cpu().long())
                tmp = out.cpu().numpy()
                preds += list(np.argmax(tmp, axis=1))
                tmps += list(F.softmax(torch.tensor(tmp), dim=1).numpy())
        self.confidence_error_score = tmps

    def predict_batch(self, log_lines: List[str]):
        tokenized = []
        regex = re.compile('[^a-zA-Z ]')
        self.log_lines = self._get_word_class_embeddings(log_lines)

        log_lines_dep = self._get_word_class_embeddings_dep(log_lines)

        for dep in log_lines_dep:
            self._predict_top_N_suitable_tokens(self._mapping_fcn(dep, inverse_data_map_))
        # self.log_lines
        for x in self.log_lines:
            tokenized.append(torch.tensor(self.tokenizer.tokenize_test(' '.join(regex.sub('', x).strip().split())))[:PAD_SIZE])
        test_dataloader = self._create_test_data_loaders(tokenized, self.pad_len, self.batch_size)

        self._predict(test_dataloader)
        scores = 1-np.array(self.confidence_error_score)[:, 1]
        # print(len(self.log_lines), len(scores), len(self.predictions))
        # print(self.predictions)
        self.results = [{"tags": tag, "linguistic_prediction":sc, "suggestions":sug} for tag, sc, sug in zip(self.log_lines, scores, self.predictions)]

        # self.results = {"prediction": scores, "tags": self.log_lines}
        return self.results

    def _inverse_map(self, x, data_map_):
        z = []
        for i in x:
            z.append(data_map_[i])
        return " ".join(z)

    def _mapping_fcn(self, x, inverse_data_map_):
        kk = []
        x = x.rsplit()
        for p in x:
            try:
                kk.append(inverse_data_map_[p])
            except Exception as e:
                pass
        return "".join(kk)

    def _predict_top_N_suitable_tokens(self, bad_data, N=100):
        output = self._lev_metric(good_data, bad_data)
        output = pd.DataFrame(output, index=[bad_data])
        output.columns = [self._inverse_map(x, data_map_) for x in output.columns]
        output.index = [self._inverse_map(x, data_map_) for x in output.index]
        predictions = []
        # for query in output.index:
        query = output.index[0]
        predictions = (self._recommendation_rules(list(output.columns[np.argsort(output.loc[query])]), query=query, N=10), query)
        self.predictions.append(predictions)

    def _recommendation_rules(self, rules, query, N=10, minimal=4):
        ## If you think off different ways how to filter out rules you can implement a separate method to do so
        rules = self._minimal_rule_size(rules, query)
        rules = self._shorten_rule(rules, N=N)
        return rules

    def _shorten_rule(self, rules, N):
        return rules[:N]

    def _minimal_rule_size(self, rules, query, minimal=4):
        new_rules = []
        for rule in rules:
            if len(rule.rsplit())>len(query.rsplit()):
                new_rules.append(rule)
        return new_rules

    def get_scores(self):
        return self.confidence_error_score

# !!! Autogenerated !!! This will be used as a simple functionality test
if __name__ == "__main__":
    q = QulogLinguisticAttention()
    s = "very good Neutron connects to Nova"
    p1 = q.predict(s)
    p2 = q.predict_batch([s, s, s])
    print("Prediction for '{}': {}".format(s, p1))
    print("Prediction for '{}': {}".format(s, p2))