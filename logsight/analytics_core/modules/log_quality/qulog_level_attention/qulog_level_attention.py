import os
import pickle
import sys
import torch
import numpy as np
import torch.nn.functional as F
print("PATH", os.getcwd())
sys.path.append("qulog_level_attention/classes")

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

from typing import List
from .classes.tokenizer import *

# !!! Autogenerated !!! Do not change. Path where this python file is located
home = os.path.dirname(os.path.realpath(__file__))
PAD_SIZE = 32

# !!! Autogenerated !!! Do not change the class name
class QulogLevelAttention:
    """
    __author__: Bogatinovski Jasmin
    __email__: jasmin.bogatinovski@gmail.com

    This class implements the log level prediction functionality of the QuLog method (Stands for Quality Log Message).
    It implements a neural network to predict the log level of the given input log message.
    The supported log levels are: [INFO and ERROR].
    The codding for the log messages is: {INFO: 1, ERROR:0}.

    It provides 3 types of output:
    1) maximal softmax score prediction.
    2) thresholded prediction.
    3) uncertainty estimate for an individual log message. Simple terms: what is the confidence that a given log
    the message is an error.

    REQUIREMENTS:
    1) training model with a name model.pth stored locally together with this source file.
    2) tokenizer corresponding to the trained model with a name tokenizer.pickle stored locally together
    with this source file.

    NOTES:
    1) pad_len (mandatory parameters) is a parameter corresponding to the model size.
    Consult the model owner for its value (or find it by yourself from the nn.Embedding layers of the model);
    2) batch_size (optional) is a parameter of arbitrary value. It controls the speed of processing the log messages.
    If you have many log messages you can increase this parameter.

    """

    def __init__(self, threshold=0.9):
        """
        :param threshold: These parameters are used to put a threshold over the uncertainty estimate score.
        The experiments performed on open-source projects on source code logs and production code logs suggest that
        values around 0.9 - 0.98 can improve the performance of the maximal probability estimator.
        In case both decision procedures lead to different results, exploit further the situation.
        """
        # !!! Autogenerated !!! Do not change.
        with open("./qulog_level_attention/tokenizer.pickle", 'rb') as file:
            self.tokenizer = pickle.load(file)
        self.model = self.load(os.path.join(home, 'model.pth'))
        self.pad_len = 32
        self.batch_size = 256
        self.TH = threshold

    # !!! Autogenerated !!! Do not change the method name.
    def load(self, model_file_path):
        '''TODO Implement'''
        return torch.load(model_file_path, map_location=torch.device('cpu'))

    # !!! Autogenerated !!! Do not change the method name.
    def predict(self, log_line: str):
        '''TODO Implement'''
        return self.predict_batch([log_line])[0]
    # !!! Autogenerated !!! Do not change the method name.

    def _create_test_data_loaders(self, load_test, pad_len, batch_size):
        test_data = TensorDataset(torch.tensor(self._get_padded_data(load_test), dtype=torch.int32))
        test_sampler = SequentialSampler(test_data)
        test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)
        return test_dataloader

    # def _get_padded_data(self, data, pad_len):
    #     padded_seq = pad_sequences(data, maxlen=pad_len, dtype="long", truncating="post", padding="post")
    #     return padded_seq

    def _get_padded_data(self, tokenized):
        padded = torch.nn.utils.rnn.pad_sequence(tokenized, batch_first=True)
        return padded

    def _threshold_classifier(self, score_estimates_bad):
        return np.where(score_estimates_bad > self.TH, 0, 1)

    def _predict(self, dataloader):
        self.model.eval()
        preds = []
        tmps = []
        with torch.no_grad():
            for i, batch in enumerate(dataloader):
                load = batch[0]
                out = self.model.forward(load.cpu().long())
                tmp = out.cpu().numpy()
                preds += list(np.argmax(tmp, axis=1))
                tmps += list(F.softmax(torch.tensor(tmp), dim=1).numpy())
        th_preds = self._threshold_classifier(np.array(tmps))
        self.confidence_error_score = tmps
        return preds, th_preds

    def predict_batch(self, log_lines: List[str]):
        '''TODO Implement'''
        tokenized = []
        regex = re.compile('[^a-zA-Z ]')
        for x in log_lines:
            tokenized.append(torch.tensor(self.tokenizer.tokenize_test(' '.join(regex.sub('', x).strip().split())))[:PAD_SIZE])
        test_dataloader = self._create_test_data_loaders(tokenized, self.pad_len, self.batch_size)
        predictions_moc, predictions_th = self._predict(test_dataloader)
        return predictions_th[:, 0]

    def get_scores(self):
        return self.confidence_error_score

# !!! Autogenerated !!! This will be used as a simple functionality test
if __name__ == "__main__":
    q = QulogLevelAttention()
    s = "Hello world"
    p1 = q.predict(s)
    p2 = q.predict_batch([s])
    print("Prediction for '{}': {}".format(s, p1))
    print("Prediction for '{}': {}".format(s, p2[0]))